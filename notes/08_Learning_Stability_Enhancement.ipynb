{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6494e593",
            "metadata": {},
            "source": [
                "# 08. Learning Staibility and Perfomance Enhancement\n",
                "## 학습 안정 및 성능 향상 기법"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4eaa61e0",
            "metadata": {},
            "source": [
                "### 기울기 소실(Vanishin Gradient)\n",
                "- 시그모이드 함수 접선의 기울기의 최대값은 1/4\n",
                "- 큰 입력 값에서 기울기 거의 0\n",
                "- `기울기가 0에 가까워 진다`는 의미 = **기울기가 작아져 학습이 제대로 이루어지지 않는다**\n",
                "- 가중치를 업데이트한다 = 학습한다\n",
                "- 문제\n",
                "\t- 초기층 학습 어려움 \n",
                "\t- 학습 속도 저하\n",
                "\t- 정확도 하락 : 앞쪽 데이터가 품질이 저하인데 뒤에 데이터가 정확하더라고 품질이 저하됨\n",
                "- 해결하기 위해서 활성화 함수를 바꿔줘야함\n",
                "\n",
                "#### ReLU(Rectified Linear Unit) 함수\n",
                "- 입력이 0보다 크면 그대로 출력, 0이하이면 0을 출력하는 활성화 함수\n",
                "\n",
                "#### Leakly ReLU함수\n",
                "- 입력이 0보다 크면 그대로 출력하고, 0 이하이면 작은 기울기(α)를 곱해 출력하는 함수"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "30676208",
            "metadata": {},
            "source": [
                "### 배치 정규화, 레이어 정규화\n",
                "#### BN(Batch Normalization)\n",
                "- 배치 정규화\n",
                "- 한 노드에 들어오는 입력값을 정규화\n",
                "- 어디에 = 평균, 얼만큼 세게 = 분산\n",
                "- 학습 과정에서 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도 `각 배치별로 평균과 분산을 이용해 정규화하는 것`\n",
                "- 평균은 0, 표준 편차는 1로 데이터의 분포를 조정\n",
                "\t- 재배치 하되, 순서는 바꾸지 않음\n",
                "- batch의 크기에 영향을 받음\n",
                "- 평균과 분산은 가중치를 업데이트 할 때 같이 찾음\n",
                "- `기울기 소실을 완화`하는데 도움이 됨\n",
                "- `이미지 처리`\n",
                "\n",
                "#### LN(Layer Normalization)\n",
                "- 레이어 정규화\n",
                "- 한 층에 들어오는 입력값을 정규화\n",
                "- RNN 모델에 효과적 = `자연어 처리`에 효과적"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "456036cb",
            "metadata": {},
            "source": [
                "### 과적합 (Overfitting)\n",
                "- 학습 데이터에 대해 모델이 지나치게 적합되어 새로운 데이터나 테스트 데이터에서 성능이 저하되는 현상\n",
                "- 원인\n",
                "\t- `학습 시간 과다` : 학습을 지나치게 오래 진행하면 모델이 데이터의 작은 변동까지 학습하게 되어 과적합 발생\n",
                "    - `복잡한 모델 구조` : 학습 데이터의 패턴을 과도하게 학습하는 복잡한 모델(과도한 파라미터)로 인해 발생\n",
                "    - `학습 데이터 부족` : 데이터 양이 적거나, 특정 패턴에 편중되어 있어 모델이 일반화된 패턴을 학습하지 못함 ➡️ 다양하게 많아야함\n",
                "- 해결방법\n",
                "\t- 데이터 증강 : 데이터의 다양성을 높이기 위해 데이터를 변형하거나 추가로 생성\n",
                "    - 데이터 확대 : 학습 데이터의 양을 늘려 모델이 더 일반화된 패턴을 학습할 수 있도록 함\n",
                "    - Regularization\n",
                "    - Drop-Out : 뉴런이 너무 커졌을 때 일부 뉴런을 비활성화 시켜 너무 많은 영향을 받지 않게 하는 효과, `과적합을 방지`하고 모델의 일반화 성능을 향상시키기 위해 고안된 정규화 기법"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75404609",
            "metadata": {},
            "source": [
                "### Regularization\n",
                "- 가중치(weight)의 크기를 억제해서 모델이 과적합(overfitting)하지 않도록 잡아주는 역할\n",
                "- L1, L2 정규화\n",
                "-  Norm : 벡터의 크기를 측정하는 방법, 두 벡터 사이의 거리를 측정하는 방법\n",
                "\t- L1 Norm : 두 벡터 각 원소들 차이의 `절대값의 합`, 일괄적으로 압박 → 일부는 아예 0이 됨\n",
                "\t- L2 Norm: 두 벡터 사이의 직선 거리(`제곱의 합`), 큰 가중치를 특히 억제 → 균형 잡힌 가중치 분포\n",
                "- L1 Regularization : `불필요한 Weight를 0으로만들어버려서` Feature selection의 효과\n",
                "- L2 Regularization : 가중치가 너무 크기 않은 방향으로 학습하게 함, `불필요한 Feature를 0에 가깝게 만드는 특성`"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
