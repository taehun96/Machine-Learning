{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb9c19e",
   "metadata": {},
   "source": [
    "# 06. Regresstion & Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e990d2",
   "metadata": {},
   "source": [
    "### 회귀와 경사하강법\n",
    "#### 회귀\n",
    "- 의미변화\n",
    "\t- 데이터가 평균으로 돌아가는 경향\n",
    "\t➡️ 변수 간의 관계를 모델링하여 예측하는 방법\n",
    "\t- `함수를 모델링하여 예측하는 것`\n",
    "- 연속적인 값(실수형 데이터)을 예측하기 위해 사용\n",
    "- 주요 목표 : 최적의 회귀 계수를 찾아내는 것\n",
    "\n",
    "#### 선형회귀\n",
    "- 종속 변수 y와 하나 이상의 독립 변수 x와의 선형 상관관계를 모델링하는 기법\n",
    "- 데이터 속에서 일정한 패턴(규칙)을 찾아내고, 이를 직선 방정식으로 표현\n",
    "\n",
    "#### 오차\n",
    "- 모델이 예측한 값(y^)과 실제 값(y)의 차이를 나타내는 지표\n",
    "- 오차가 0에 가까울수록 정확도가 높다\n",
    "- `선형회귀와 오차`\n",
    "\t- 임의의 W와 b를 설정하고 오차를 줄이도록 학습\n",
    "    - `오차를 최소화`하는 W와 b를 찾는 것이 목표\n",
    "- `MSE(Mean Squared Error)`\n",
    "\t- 평균제곱오차 \n",
    "\t- 큰 오차를 줄이고 싶을때 사용\n",
    "\t- **손실에 민감하기 때문에 더 자주사용**\n",
    "- `MAE(Mean Abosolute Error)`\n",
    "\t- 평균절대오차 \n",
    "\t- 평균적인 오차 크기를 직관적으로 보고 싶을 때 사용\n",
    "    - 틔는 데이터가 많을때 사용\n",
    "\n",
    "#### 손실 함수\n",
    "- 머신러닝에서 모델의 성능을 측정하고 학습시키는데 사용되는 함수\n",
    "- 입력되는 가중치와 편향을 계속 바꿈으로 인해 출력되는 손실 값이 바뀌고 그로인해 함수라고 부를 수 밖에 없음\n",
    "- `손실함수의 값을 최소화`하는데 목적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3811b57",
   "metadata": {},
   "source": [
    "#### 경사하강법\n",
    "- 최솟값을 어떻게 찾아 갈 것인가?\n",
    "- 경사하강법을 통해 최소값을 찾아감\n",
    "- 방법\n",
    "\t- 가중치와 편향을 업데이트\n",
    "    - 손실 함수의 기울기 계산(그라디언트 찾음)\n",
    "    - 기울기(가파른 방향)의 반대 방향으로 이동\n",
    "    - 학습률 적용\n",
    "    - 반복 : 손실이 점점 줄어들고, 기울기가 거의0이 될때까지 반복\n",
    "    - 최종적으로 최솟값 근처의 최적 가중치와 편향에 도달\n",
    "\n",
    "- 학습률 : 사용자가 설정하는 하이퍼파라미터\n",
    "\n",
    "- 볼록함수(convex function)\n",
    "    - 아래로 볼록한 함수, 경사하강법은 볼록함수에서 잘 작동함\n",
    "    - 경사하강법은 Convex function에서 잘 작동함\n",
    "    - 단점 \n",
    "\t    - Non-convex function 같은 경우 Local min(지역 최솟값) 문제 생길 수 있음\n",
    "        - 데이터가 클 경우 느릴 수 있음\n",
    "\n",
    "- 경사하강법 문제\n",
    "    - Local minimum : 전역 최소값이 있지만 지역 최솟값에 안착하는 문제\n",
    "    - 비등방성 함수\n",
    "    - Saddle point(안장점) : 최댓값인지 최솟값인지 구분할 수 없음\n",
    "    - Overshooting : 학습률이 너무 커서 매개변수가 최솟값을 지나쳐버리는 현상\n",
    "\n",
    "- 경사하강법 종류\n",
    "    - `배치 경사 하강법`(BGD, Batch Gradient Descent)\n",
    "        - 모든 에러를 고려해서 계산\n",
    "        - 전체 데이터\n",
    "        - 안정적이지만 계산이 느림\n",
    "        - Local minimum에 빠질 확률이 높음\n",
    "\n",
    "    - `확률적 경사 하강법`(SGD, Stochastic Gradient Descent)\n",
    "        - 에러를 하나씩 고려해서 계산\n",
    "        - 샘플 1개\n",
    "        - 계산 속도 빠름\n",
    "        - `노이즈가 포함`된 학습 경로가 `안장점이나 지역 최솟값에서 빠져나오는데` 도움을 줄 수 있음\n",
    "        - 학습 경로가 불안정, 진동이 발생\n",
    "\n",
    "    - `미니 배치 경사 하강법`(MGD, Mini-batch Gradient Descent)\n",
    "        - 몇 개 골라 뽑아서 계산\n",
    "        - 데이터 일부 묶음\n",
    "        - 배치 크기에 따라 성능이 달라질 수 있음\n",
    "\n",
    "- Optimizer\n",
    "    - SGD의 단점을 개선\n",
    "    - 모델의 학습 과정을 제어하여, 손실 함수를 최소화하기 위해 사용되는 알고리즘 또는 방법론\n",
    "    - Momentum : 기울기 업데이트에 `이전 업데이트 방향의 관성을 추가`하여 더 빠르고 안정적으로 최적값에 도달하도록 돕는 방법\n",
    "    - RMSProp : 기울기가 큰 방향에서는 학습률 줄임, 기울기가 작은 방향에서는 증가 즉, 보폭 결정 \n",
    "    - Adam : RMSProp + Momentum, 방향도 스텝사이즈도 적절하게"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
